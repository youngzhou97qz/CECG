import keras
import os
import collections
import copy
import math
import numpy as np
import tensorflow as tf
import random as rn
import matplotlib.pyplot as plt
from tqdm import tqdm
from itertools import groupby
from keras import *
from keras.layers import *
from keras.models import *
from keras.callbacks import *
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

# recurrent
os.environ['PYTHONHASHSEED'] = '0'
np.random.seed(42)
rn.seed(12345)
session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
tf.set_random_seed(1234)
sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
K.set_session(sess)

# parameter
dim = 64
dropout = 0
maxlen = 32
batch_size = 4096
epochs = 1000
train_num = 1600000

s1 = '咆哮 段子 之 在 香港 工作 的 人 你们 伤 不 起 啊 ！'

# read data
questions, answers, a_id = [], [], []

f = open('/home/yzhou/STC3/data/questions.txt','r',encoding='gbk')
lines = f.readlines()
for line in lines:
    line = line.strip()
    questions.append(line)
f.close()

f = open('/home/yzhou/STC3/data/answers.txt','r',encoding='gbk')
lines = f.readlines()
for line in lines:
    line = line.strip()
    answers.append(line)
f.close()

f = open('/home/yzhou/STC3/data/Sentiment classification.json','r',encoding='UTF-8')
load_sent = json.load(f)
weibo,weibo_id = [],[]
for i in range(len(load_sent)):
    weibo.append(load_sent[i][0])
    weibo_id.append(load_sent[i][1])
f.close()

print(len(questions),len(answers),len(weibo),len(weibo_id))

# judging chinese
def check_contain_chinese(check_str):
    for ch in check_str:
        if u'\u4e00' <= ch <= u'\u9fff':
            return True
    return False

# delete non-Chinese
i = 0
while 1:
    j = len(questions)
    if i >= j:
        break
    if check_contain_chinese(questions[i]) and check_contain_chinese(answers[i]):
        i += 1
    else:
        questions.pop(i)
        answers.pop(i)

i = 0
while 1:
    j = len(weibo)
    if i >= j:
        break
    if check_contain_chinese(weibo[i]):
        i += 1
    else:
        weibo.pop(i)
        weibo_id.pop(i)

print(len(questions),len(answers),len(weibo),len(weibo_id))

# standardization
def pre_process(text):
    for j in range(len(text)):
        
        # expression
        text[j] = text[j].replace('[ [', '[').replace('] ]', ']')
        zuo, you = 0, 0
        g = len(text[j])-1
        while 1:
            if text[j][g] ==  ']':
                you = g
            elif text[j][g] == '[':
                zuo = g
                for h in range(you,zuo,-1):
                    if text[j][h] == ' ':
                        you -= 1
                        text[j] = text[j][:h] + text[j][h+1:]
            if g == 0:
                break
            else:
                g -= 1

        # duplicate words
        text[j] = text[j].split(' ')
        text[j] = [x[0] for x in groupby(text[j])]
        for index, string in enumerate(text[j]):
            if len(string) > 3:
                char_list = list(string)
                for i in range(len(string)-3):
                    if char_list[len(string)-1-i] == char_list[len(string)-2-i] and char_list[len(string)-1-i] == char_list[len(string)-3-i] and char_list[len(string)-1-i] == char_list[len(string)-4-i]:
                        char_list.pop(len(string)-1-i)
                text[j][index] = "".join(char_list)
        g = 0
        while 1:
            if text[j][g] == '':
                text[j].pop(g)
            else:
                g += 1
            if g > len(text[j])-1:
                break
        
        temp = ''.join(text[j])
        text[j] = list(temp)
    return text
    
    # standardization
questions = pre_process(questions)
answers = pre_process(answers)
weibo = pre_process(weibo)
print(len(questions),len(answers),len(weibo))

# initial dictionary
char_ques,char_answ = [],[]
for i in range(len(questions)):
    for j in range(len(questions[i])):
        char_ques.append(questions[i][j])
        
for i in range(len(answers)):
    for j in range(len(answers[i])):
        char_answ.append(answers[i][j])
        
ques_dict = collections.Counter(char_ques)
answ_dict = collections.Counter(char_answ)
print(len(ques_dict),len(ques_dict))

# word frequency
rest_ques = dict(filter(lambda x: (x[1] > 50 and (x[0] >= '\u4e00' and x[0] <= '\u9fff')) or (x[1] > 250 and (x[0] < '\u4e00' or x[0] > '\u9fff')), ques_dict.items()))
print('问词典数：',len(rest_ques))

rest_answ = dict(filter(lambda x: (x[1] > 250 and (x[0] >= '\u4e00' and x[0] <= '\u9fff')) or (x[1] > 500 and (x[0] < '\u4e00' or x[0] > '\u9fff')), answ_dict.items()))
print('答词典数：',len(rest_answ))

# Reconstructed corpus
train_ques,train_answ = [],[]
for i in tqdm(range(len(questions))):
    temp_ques, temp_answ = '','<开始> '
    for j in range(len(questions[i])):
        if questions[i][j] in rest_answ.keys():
            temp_ques += questions[i][j]
            if j != len(questions[i])-1:
                temp_ques += ' '
        else:
            temp_ques += '<未知>'
            if j != len(questions[i])-1:
                temp_ques += ' '
    train_ques.append(temp_ques)
    for j in range(len(answers[i])):
        if answers[i][j] in rest_answ.keys():
            temp_answ += answers[i][j]
        else:
            temp_answ += '<未知>'
        temp_answ += ' '
    temp_answ += '<结束>'
    train_answ.append(temp_answ)
    
# Reconstructed corpus
train_weibo = []
for i in tqdm(range(len(weibo))):
    temp_weibo = '<开始> '
    for j in range(len(weibo[i])):
        if weibo[i][j] in rest_answ.keys():
            temp_weibo += weibo[i][j]
        else:
            temp_weibo += '<未知>'
        temp_weibo += ' '
    temp_weibo += '<结束>'
    train_weibo.append(temp_weibo)
    
# serialization
tokenizer = Tokenizer(filters='')
tokenizer.fit_on_texts(train_ques+train_answ+train_weibo)
ques_text = tokenizer.texts_to_sequences(train_ques)
answ_text = tokenizer.texts_to_sequences(train_answ)
weibo_text = tokenizer.texts_to_sequences(train_weibo)

dic = tokenizer.word_index
dic['<填充>'] = 0  #'<填充>': 0, '<开始>': 1, '<结束>': 2, '<未知>': 21
ind ={value:key for key, value in dic.items()}

# data for train and valid
input_ques,input_answ,input_label,input_weibo,output_answord,output_weiword,temp_ques,temp_label = [],[],[],[],[],[],[],[]
temp_ques = pad_sequences(ques_text, maxlen=maxlen, truncating='post')
for i in range(len(weibo_id)):
    temp_id = np.ones(maxlen, dtype='int32') * int(weibo_id[i])
    temp_label.append(temp_id)
for i in tqdm(range(len(answ_text))):
    temp_answ = []
    for j in range(maxlen):
            temp_answ.append(0)
    for j in range(maxlen):
        temp_answ.pop(0)
        temp_answ.append(answ_text[i][j])
        temp = copy.deepcopy(temp_answ)
        input_answ.append(temp)
        output_answord.append(answ_text[i][j+1])
        input_ques.append(temp_ques[i])
        if answ_text[i][j+1] == 2:
            break
for i in tqdm(range(len(weibo_text))):
    temp_weibo = []
    for j in range(maxlen):
            temp_weibo.append(0)
    for j in range(maxlen):
        temp_weibo.pop(0)
        temp_weibo.append(weibo_text[i][j])
        temp = copy.deepcopy(temp_weibo)
        input_weibo.append(temp)
        output_weiword.append(weibo_text[i][j+1])
        input_label.append(temp_label[i])
        if weibo_text[i][j+1] == 2:
            break
input_ques = np.asarray(input_ques)
input_answ = np.asarray(input_answ)
output_answord = np.asarray(np.expand_dims(output_answord, axis=-1))

input_label = np.asarray(input_label)
input_weibo = np.asarray(input_weibo)
output_weiword = np.asarray(np.expand_dims(output_weiword, axis=-1))

# beam
def gen_resp(post,emotion,emotionrate=0.5,topk=5):
    test_post = pre_process([post])[0]
    for i in range(len(test_post)):
        if test_post[i] in dic.keys():
            test_post[i] = dic[test_post[i]]
        else:
            test_post[i] = dic['<未知>']
    test_post = pad_sequences([test_post], maxlen=maxlen, truncating='post')[0]
    test_post = np.array([test_post] * topk)

    test_emot = np.ones(maxlen, dtype=int) * int(emotion)
    test_emot = np.array([test_emot] * topk)

    temp_resp = []
    for j in range(maxlen):
        temp_resp.append(0)
    temp = 1
    temp_resp.pop(0)
    temp_resp.append(temp)
    test_resp = np.array([np.asarray(temp_resp)] * topk)
    
    scores = [0] * topk
    n_score = []
    for i in range(maxlen):
        proba_1 = model.predict([np.asarray(test_emot),np.asarray(test_resp)])
        proba_2 = model_2.predict([np.asarray(test_post),np.asarray(test_resp)])
        proba = emotionrate * proba_1 + (1 - emotionrate) * proba_2
        char_pred = np.argsort(proba)[:,-topk:]
        temp_resp = test_resp.tolist()
        if i == 0:
            for j in range(topk):
                temp_resp[j].pop(0)
                temp_resp[j].append(char_pred[j][-1-j])
                n_score.append(proba[0][char_pred[j][-1-j]])
                test_resp = np.asarray(temp_resp)
        else:
            nn_score,tt_resp = [],[]
            for j in range(topk):
                for k in range(topk):
                    nn_score.append(n_score[j]+proba[j][char_pred[j][-1-k]])
                    tt_resp.append(char_pred[j][-1-k])
            nn_pred = np.argsort(nn_score)[-topk:]
            for j in range(topk):
                k = nn_pred[-1-j] // topk
                tempt_resp = test_resp.tolist()
                tempt_resp[k].pop(0)
                tempt_resp[k].append(tt_resp[nn_pred[-1-j]])
                temp_resp[j] = copy.deepcopy(tempt_resp[k])
                n_score[j] = nn_score[nn_pred[-1-j]]
            test_resp = np.asarray(temp_resp)
    test_resp = test_resp[0].tolist()
    for i in range(len(test_resp)):
        test_resp[i] = ind[test_resp[i]]
        if test_resp[i] == '<结束>':
            break
    text_pred = ''.join(test_resp[0:i+1])
    return text_pred.lstrip('<填充>').lstrip('<开始>').rstrip('<结束>')

class Evaluate(Callback):
    def __init__(self):
        self.lowest = 0
    def on_epoch_end(self, epoch, logs=None):
        print('Post：',s1)
        print('Other：',gen_resp(s1,0,emotionrate=0.2))
        print('Like：',gen_resp(s1,1,emotionrate=0.2))
        print('Sadness：',gen_resp(s1,2,emotionrate=0.2))
        print('Disgust：',gen_resp(s1,3,emotionrate=0.2))
        print('Anger：',gen_resp(s1,4,emotionrate=0.2))
        print('Happiness：',gen_resp(s1,5,emotionrate=0.2))

dim = 256
# model
lab1 = 'model_219'

embed = Embedding(len(dic), dim)

input_emot = Input(shape=(None,), name='input_emot')
w = Embedding(6, dim, name='embed_label')(input_emot)

input_sent = Input(shape=(None,), name='input_sent')
z = embed(input_sent)
z = BatchNormalization()(z)
z = LSTM(dim*2, return_sequences=True, dropout=dropout, name='slstm_weibo')(z)

merge_weibo = concatenate([w, z], name='merge_weibo')
wz = BatchNormalization()(merge_weibo)
wz = Bidirectional(LSTM(dim, return_sequences=False, dropout=dropout), name='blstm_weibo')(wz)
outpt_weibo = Dense(len(dic), activation='softmax', name='outpt_weibo')(wz)

model = Model(inputs=[input_emot, input_sent], outputs=outpt_weibo)
model.summary()

# compile
model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.Adam(), metrics=[metrics.sparse_categorical_accuracy])

# callback
filepath = '/home/yzhou/STC3/weight/' + lab1 + '.hdf5'
checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=True, mode='min')
plateau = ReduceLROnPlateau(factor=0.1, patience=2, verbose=1, min_lr=0.00002)
stop = EarlyStopping(patience=5, verbose=1)
callbacks_list = [checkpoint,plateau,stop]

# train
history1 = model.fit(x=[input_label,input_weibo], y=output_weiword, batch_size=512, epochs=1000, validation_split=0.1, callbacks=callbacks_list)

dim = 256
# model
lab2 = 'model_221'

embed = Embedding(len(dic), dim)

input_post = Input(shape=(None,), name='input_post')
x = embed(input_post)
x = BatchNormalization()(x)
x = Bidirectional(LSTM(dim, return_sequences=True, dropout=dropout), name='blstm_post')(x)

input_resp = Input(shape=(None,), name='input_resp')
y = embed(input_resp)
y = BatchNormalization()(y)
y = LSTM(dim, return_sequences=True, dropout=dropout, name='slstm_resp')(y)

merge_resp = concatenate([x, y], name='merge_resp')
xy = BatchNormalization()(merge_resp)
xy = Bidirectional(LSTM(dim, return_sequences=False, dropout=dropout), name='blstm_resp')(xy)
outpt_resp = Dense(len(dic), activation='softmax', name='outpt_resp')(xy)

model_2 = Model(inputs=[input_post, input_resp], outputs=outpt_resp)
model_2.summary()

# compile
model_2.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.Adam(), metrics=[metrics.sparse_categorical_accuracy])

# callback
filepath = '/home/yzhou/STC3/weight/' + lab2 + '.hdf5'
checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=True, mode='min')
plateau = ReduceLROnPlateau(factor=0.1, patience=2, verbose=1, min_lr=0.00002)
stop = EarlyStopping(patience=5, verbose=1)
evaluator = Evaluate()
callbacks_list = [checkpoint,plateau,stop,evaluator]

# train
history2 = model_2.fit(x=[input_ques,input_answ], y=output_answord, batch_size=1024, epochs=1000, validation_split=0.1, callbacks=callbacks_list)
